{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cbfec95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installation complete.\n"
     ]
    }
   ],
   "source": [
    "# @title Step 0: Setup and Installation\n",
    "# Install ADK and LiteLLM for multi-model support\n",
    "\n",
    "!pip install google-adk -q\n",
    "!pip install litellm -q\n",
    "\n",
    "print(\"Installation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37678e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# @title Import necessary libraries\n",
    "import os\n",
    "import asyncio\n",
    "from google.adk.agents import Agent\n",
    "from google.adk.models.lite_llm import LiteLlm # For multi-model support\n",
    "from google.adk.sessions import InMemorySessionService\n",
    "from google.adk.runners import Runner\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "import warnings\n",
    "# Ignore all warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a74d48eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI API Key set: Yes\n"
     ]
    }
   ],
   "source": [
    "# @title Configure API Keys (Replace with your actual keys!)\n",
    "\n",
    "# --- IMPORTANT: Replace placeholders with your real API keys ---\n",
    "\n",
    "# [Optional]\n",
    "# OpenAI API Key (Get from OpenAI Platform: https://platform.openai.com/api-keys)\n",
    "os.environ['OPENAI_API_KEY'] = 'AIzaSyCxWKvYAy5z7kcVdVw8Leia08GvBAYEgws' # <--- REPLACE\n",
    "\n",
    "print(f\"OpenAI API Key set: {'Yes' if os.environ.get('OPENAI_API_KEY') and os.environ['OPENAI_API_KEY'] != 'YOUR_OPENAI_API_KEY' else 'No (REPLACE PLACEHOLDER!)'}\")\n",
    "\n",
    "# Configure ADK to use API keys directly (not Vertex AI for this multi-model setup)\n",
    "os.environ[\"GOOGLE_GENAI_USE_VERTEXAI\"] = \"False\"\n",
    "\n",
    "\n",
    "# @markdown **Security Note:** It's best practice to manage API keys securely (e.g., using Colab Secrets or environment variables) rather than hardcoding them directly in the notebook. Replace the placeholder strings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5be09b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Environment configured.\n"
     ]
    }
   ],
   "source": [
    "# --- Define Model Constants for easier use ---\n",
    "\n",
    "# More supported models can be referenced here: https://ai.google.dev/gemini-api/docs/models#model-variations\n",
    "MODEL_GEMINI_2_0_FLASH = \"gemini-2.0-flash\"\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/openai#openai-chat-completion-models\n",
    "MODEL_GPT_4O = \"gemini-2.5-flash\" # You can also try: gpt-4.1-mini, gpt-4o etc.\n",
    "\n",
    "# More supported models can be referenced here: https://docs.litellm.ai/docs/providers/anthropic\n",
    "MODEL_CLAUDE_SONNET = \"anthropic/claude-sonnet-4-20250514\" # You can also try: claude-opus-4-20250514 , claude-3-7-sonnet-20250219 etc\n",
    "\n",
    "print(\"\\nEnvironment configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f612971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tool: get_weather called for city: New York ---\n",
      "{'status': 'success', 'report': 'The weather in New York is sunny with a temperature of 25°C.'}\n",
      "--- Tool: get_weather called for city: Paris ---\n",
      "{'status': 'error', 'error_message': \"Sorry, I don't have weather information for 'Paris'.\"}\n"
     ]
    }
   ],
   "source": [
    "# @title Define the get_weather Tool\n",
    "def get_weather(city: str) -> dict:\n",
    "    \"\"\"Retrieves the current weather report for a specified city.\n",
    "\n",
    "    Args:\n",
    "        city (str): The name of the city (e.g., \"New York\", \"London\", \"Tokyo\").\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the weather information.\n",
    "              Includes a 'status' key ('success' or 'error').\n",
    "              If 'success', includes a 'report' key with weather details.\n",
    "              If 'error', includes an 'error_message' key.\n",
    "    \"\"\"\n",
    "    print(f\"--- Tool: get_weather called for city: {city} ---\") # Log tool execution\n",
    "    city_normalized = city.lower().replace(\" \", \"\") # Basic normalization\n",
    "\n",
    "    # Mock weather data\n",
    "    mock_weather_db = {\n",
    "        \"newyork\": {\"status\": \"success\", \"report\": \"The weather in New York is sunny with a temperature of 25°C.\"},\n",
    "        \"london\": {\"status\": \"success\", \"report\": \"It's cloudy in London with a temperature of 15°C.\"},\n",
    "        \"tokyo\": {\"status\": \"success\", \"report\": \"Tokyo is experiencing light rain and a temperature of 18°C.\"},\n",
    "    }\n",
    "\n",
    "    if city_normalized in mock_weather_db:\n",
    "        return mock_weather_db[city_normalized]\n",
    "    else:\n",
    "        return {\"status\": \"error\", \"error_message\": f\"Sorry, I don't have weather information for '{city}'.\"}\n",
    "\n",
    "# Example tool usage (optional test)\n",
    "print(get_weather(\"New York\"))\n",
    "print(get_weather(\"Paris\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d89cf615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_v1' created using model 'gemini-2.5-flash'.\n"
     ]
    }
   ],
   "source": [
    "# @title Define the Weather Agent\n",
    "# Use one of the model constants defined earlier\n",
    "AGENT_MODEL = MODEL_GPT_4O # Starting with Gemini\n",
    "\n",
    "weather_agent = Agent(\n",
    "    name=\"weather_agent_v1\",\n",
    "    model=AGENT_MODEL, # Can be a string for Gemini or a LiteLlm object\n",
    "    description=\"Provides weather information for specific cities.\",\n",
    "    instruction=\"You are a helpful weather assistant. \"\n",
    "                \"When the user asks for the weather in a specific city, \"\n",
    "                \"use the 'get_weather' tool to find the information. \"\n",
    "                \"If the tool returns an error, inform the user politely. \"\n",
    "                \"If the tool is successful, present the weather report clearly.\",\n",
    "    tools=[get_weather], # Pass the function directly\n",
    ")\n",
    "\n",
    "print(f\"Agent '{weather_agent.name}' created using model '{AGENT_MODEL}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8b504a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session created: App='weather_tutorial_app', User='user_1', Session='session_001'\n",
      "Runner created for agent 'weather_agent_v1'.\n"
     ]
    }
   ],
   "source": [
    "# @title Setup Session Service and Runner\n",
    "\n",
    "# --- Session Management ---\n",
    "# Key Concept: SessionService stores conversation history & state.\n",
    "# InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "session_service = InMemorySessionService()\n",
    "\n",
    "# Define constants for identifying the interaction context\n",
    "APP_NAME = \"weather_tutorial_app\"\n",
    "USER_ID = \"user_1\"\n",
    "SESSION_ID = \"session_001\" # Using a fixed ID for simplicity\n",
    "\n",
    "# Create the specific session where the conversation will happen\n",
    "session = await session_service.create_session(\n",
    "    app_name=APP_NAME,\n",
    "    user_id=USER_ID,\n",
    "    session_id=SESSION_ID\n",
    ")\n",
    "print(f\"Session created: App='{APP_NAME}', User='{USER_ID}', Session='{SESSION_ID}'\")\n",
    "\n",
    "# --- Runner ---\n",
    "# Key Concept: Runner orchestrates the agent execution loop.\n",
    "runner = Runner(\n",
    "    agent=weather_agent, # The agent we want to run\n",
    "    app_name=APP_NAME,   # Associates runs with our app\n",
    "    session_service=session_service # Uses our session manager\n",
    ")\n",
    "print(f\"Runner created for agent '{runner.agent.name}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "237af230",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Define Agent Interaction Function\n",
    "\n",
    "from google.genai import types # For creating message Content/Parts\n",
    "\n",
    "async def call_agent_async(query: str, runner, user_id, session_id):\n",
    "  \"\"\"Sends a query to the agent and prints the final response.\"\"\"\n",
    "  print(f\"\\n>>> User Query: {query}\")\n",
    "\n",
    "  # Prepare the user's message in ADK format\n",
    "  content = types.Content(role='user', parts=[types.Part(text=query)])\n",
    "\n",
    "  final_response_text = \"Agent did not produce a final response.\" # Default\n",
    "\n",
    "  # Key Concept: run_async executes the agent logic and yields Events.\n",
    "  # We iterate through events to find the final answer.\n",
    "  async for event in runner.run_async(user_id=user_id, session_id=session_id, new_message=content):\n",
    "      # You can uncomment the line below to see *all* events during execution\n",
    "      # print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\n",
    "\n",
    "      # Key Concept: is_final_response() marks the concluding message for the turn.\n",
    "      if event.is_final_response():\n",
    "          if event.content and event.content.parts:\n",
    "             # Assuming text response in the first part\n",
    "             final_response_text = event.content.parts[0].text\n",
    "          elif event.actions and event.actions.escalate: # Handle potential errors/escalations\n",
    "             final_response_text = f\"Agent escalated: {event.error_message or 'No specific message.'}\"\n",
    "          # Add more checks here if needed (e.g., specific error codes)\n",
    "          break # Stop processing events once the final response is found\n",
    "\n",
    "  print(f\"<<< Agent Response: {final_response_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cde2084d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> User Query: What is the weather like in London?\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 21\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m call_agent_async(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me the weather in New York\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m                                        runner\u001b[38;5;241m=\u001b[39mrunner,\n\u001b[0;32m     17\u001b[0m                                        user_id\u001b[38;5;241m=\u001b[39mUSER_ID,\n\u001b[0;32m     18\u001b[0m                                        session_id\u001b[38;5;241m=\u001b[39mSESSION_ID)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Execute the conversation using await in an async context (like Colab/Jupyter)\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m run_conversation()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# --- OR ---\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Uncomment the following lines if running as a standard Python script (.py file):\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m#     except Exception as e:\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m#         print(f\"An error occurred: {e}\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m, in \u001b[0;36mrun_conversation\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_conversation\u001b[39m():\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m call_agent_async(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the weather like in London?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m                                        runner\u001b[38;5;241m=\u001b[39mrunner,\n\u001b[0;32m      7\u001b[0m                                        user_id\u001b[38;5;241m=\u001b[39mUSER_ID,\n\u001b[0;32m      8\u001b[0m                                        session_id\u001b[38;5;241m=\u001b[39mSESSION_ID)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m call_agent_async(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow about Paris?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m                                        runner\u001b[38;5;241m=\u001b[39mrunner,\n\u001b[0;32m     12\u001b[0m                                        user_id\u001b[38;5;241m=\u001b[39mUSER_ID,\n\u001b[0;32m     13\u001b[0m                                        session_id\u001b[38;5;241m=\u001b[39mSESSION_ID) \u001b[38;5;66;03m# Expecting the tool's error message\u001b[39;00m\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m call_agent_async(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me the weather in New York\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m                                        runner\u001b[38;5;241m=\u001b[39mrunner,\n\u001b[0;32m     17\u001b[0m                                        user_id\u001b[38;5;241m=\u001b[39mUSER_ID,\n\u001b[0;32m     18\u001b[0m                                        session_id\u001b[38;5;241m=\u001b[39mSESSION_ID)\n",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m, in \u001b[0;36mcall_agent_async\u001b[1;34m(query, runner, user_id, session_id)\u001b[0m\n\u001b[0;32m     12\u001b[0m final_response_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAgent did not produce a final response.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# Default\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Key Concept: run_async executes the agent logic and yields Events.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# We iterate through events to find the final answer.\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mrun_async(user_id\u001b[38;5;241m=\u001b[39muser_id, session_id\u001b[38;5;241m=\u001b[39msession_id, new_message\u001b[38;5;241m=\u001b[39mcontent):\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;66;03m# You can uncomment the line below to see *all* events during execution\u001b[39;00m\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# print(f\"  [Event] Author: {event.author}, Type: {type(event).__name__}, Final: {event.is_final_response()}, Content: {event.content}\")\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# Key Concept: is_final_response() marks the concluding message for the turn.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mis_final_response():\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m event\u001b[38;5;241m.\u001b[39mcontent \u001b[38;5;129;01mand\u001b[39;00m event\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mparts:\n\u001b[0;32m     23\u001b[0m            \u001b[38;5;66;03m# Assuming text response in the first part\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\runners.py:203\u001b[0m, in \u001b[0;36mRunner.run_async\u001b[1;34m(self, user_id, session_id, new_message, run_config)\u001b[0m\n\u001b[0;32m    195\u001b[0m   \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_append_new_message_to_session(\n\u001b[0;32m    196\u001b[0m       session,\n\u001b[0;32m    197\u001b[0m       new_message,\n\u001b[0;32m    198\u001b[0m       invocation_context,\n\u001b[0;32m    199\u001b[0m       run_config\u001b[38;5;241m.\u001b[39msave_input_blobs_as_artifacts,\n\u001b[0;32m    200\u001b[0m   )\n\u001b[0;32m    202\u001b[0m invocation_context\u001b[38;5;241m.\u001b[39magent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_find_agent_to_run(session, root_agent)\n\u001b[1;32m--> 203\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m invocation_context\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mrun_async(invocation_context):\n\u001b[0;32m    204\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m event\u001b[38;5;241m.\u001b[39mpartial:\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_service\u001b[38;5;241m.\u001b[39mappend_event(session\u001b[38;5;241m=\u001b[39msession, event\u001b[38;5;241m=\u001b[39mevent)\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\agents\\base_agent.py:147\u001b[0m, in \u001b[0;36mBaseAgent.run_async\u001b[1;34m(self, parent_context)\u001b[0m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mend_invocation:\n\u001b[0;32m    145\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 147\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_async_impl(ctx):\n\u001b[0;32m    148\u001b[0m   \u001b[38;5;28;01myield\u001b[39;00m event\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ctx\u001b[38;5;241m.\u001b[39mend_invocation:\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\agents\\llm_agent.py:275\u001b[0m, in \u001b[0;36mLlmAgent._run_async_impl\u001b[1;34m(self, ctx)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_async_impl\u001b[39m(\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28mself\u001b[39m, ctx: InvocationContext\n\u001b[0;32m    274\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncGenerator[Event, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[1;32m--> 275\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_llm_flow\u001b[38;5;241m.\u001b[39mrun_async(ctx):\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__maybe_save_output_to_state(event)\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py:282\u001b[0m, in \u001b[0;36mBaseLlmFlow.run_async\u001b[1;34m(self, invocation_context)\u001b[0m\n\u001b[0;32m    280\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m    281\u001b[0m   last_event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 282\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_one_step_async(invocation_context):\n\u001b[0;32m    283\u001b[0m     last_event \u001b[38;5;241m=\u001b[39m event\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m event\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py:314\u001b[0m, in \u001b[0;36mBaseLlmFlow._run_one_step_async\u001b[1;34m(self, invocation_context)\u001b[0m\n\u001b[0;32m    307\u001b[0m \u001b[38;5;66;03m# Calls the LLM.\u001b[39;00m\n\u001b[0;32m    308\u001b[0m model_response_event \u001b[38;5;241m=\u001b[39m Event(\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mid\u001b[39m\u001b[38;5;241m=\u001b[39mEvent\u001b[38;5;241m.\u001b[39mnew_id(),\n\u001b[0;32m    310\u001b[0m     invocation_id\u001b[38;5;241m=\u001b[39minvocation_context\u001b[38;5;241m.\u001b[39minvocation_id,\n\u001b[0;32m    311\u001b[0m     author\u001b[38;5;241m=\u001b[39minvocation_context\u001b[38;5;241m.\u001b[39magent\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m    312\u001b[0m     branch\u001b[38;5;241m=\u001b[39minvocation_context\u001b[38;5;241m.\u001b[39mbranch,\n\u001b[0;32m    313\u001b[0m )\n\u001b[1;32m--> 314\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m llm_response \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_llm_async(\n\u001b[0;32m    315\u001b[0m     invocation_context, llm_request, model_response_event\n\u001b[0;32m    316\u001b[0m ):\n\u001b[0;32m    317\u001b[0m   \u001b[38;5;66;03m# Postprocess after calling the LLM.\u001b[39;00m\n\u001b[0;32m    318\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_postprocess_async(\n\u001b[0;32m    319\u001b[0m       invocation_context, llm_request, llm_response, model_response_event\n\u001b[0;32m    320\u001b[0m   ):\n\u001b[0;32m    321\u001b[0m     \u001b[38;5;66;03m# Update the mutable event id to avoid conflict\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     model_response_event\u001b[38;5;241m.\u001b[39mid \u001b[38;5;241m=\u001b[39m Event\u001b[38;5;241m.\u001b[39mnew_id()\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\flows\\llm_flows\\base_llm_flow.py:539\u001b[0m, in \u001b[0;36mBaseLlmFlow._call_llm_async\u001b[1;34m(self, invocation_context, llm_request, model_response_event)\u001b[0m\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    535\u001b[0m   \u001b[38;5;66;03m# Check if we can make this llm call or not. If the current call pushes\u001b[39;00m\n\u001b[0;32m    536\u001b[0m   \u001b[38;5;66;03m# the counter beyond the max set value, then the execution is stopped\u001b[39;00m\n\u001b[0;32m    537\u001b[0m   \u001b[38;5;66;03m# right here, and exception is thrown.\u001b[39;00m\n\u001b[0;32m    538\u001b[0m   invocation_context\u001b[38;5;241m.\u001b[39mincrement_llm_call_count()\n\u001b[1;32m--> 539\u001b[0m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m llm_response \u001b[38;5;129;01min\u001b[39;00m llm\u001b[38;5;241m.\u001b[39mgenerate_content_async(\n\u001b[0;32m    540\u001b[0m       llm_request,\n\u001b[0;32m    541\u001b[0m       stream\u001b[38;5;241m=\u001b[39minvocation_context\u001b[38;5;241m.\u001b[39mrun_config\u001b[38;5;241m.\u001b[39mstreaming_mode\n\u001b[0;32m    542\u001b[0m       \u001b[38;5;241m==\u001b[39m StreamingMode\u001b[38;5;241m.\u001b[39mSSE,\n\u001b[0;32m    543\u001b[0m   ):\n\u001b[0;32m    544\u001b[0m     trace_call_llm(\n\u001b[0;32m    545\u001b[0m         invocation_context,\n\u001b[0;32m    546\u001b[0m         model_response_event\u001b[38;5;241m.\u001b[39mid,\n\u001b[0;32m    547\u001b[0m         llm_request,\n\u001b[0;32m    548\u001b[0m         llm_response,\n\u001b[0;32m    549\u001b[0m     )\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;66;03m# Runs after_model_callback if it exists.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\models\\google_llm.py:88\u001b[0m, in \u001b[0;36mGemini.generate_content_async\u001b[1;34m(self, llm_request, stream)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_content_async\u001b[39m(\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28mself\u001b[39m, llm_request: LlmRequest, stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     78\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AsyncGenerator[LlmResponse, \u001b[38;5;28;01mNone\u001b[39;00m]:\n\u001b[0;32m     79\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a request to the Gemini model.\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \n\u001b[0;32m     81\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m    LlmResponse: The model response.\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 88\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_preprocess_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mllm_request\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_append_user_content(llm_request)\n\u001b[0;32m     90\u001b[0m   logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m     91\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSending out request, model: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, backend: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m, stream: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     92\u001b[0m       llm_request\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m     93\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_backend,\n\u001b[0;32m     94\u001b[0m       stream,\n\u001b[0;32m     95\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\models\\google_llm.py:267\u001b[0m, in \u001b[0;36mGemini._preprocess_request\u001b[1;34m(self, llm_request)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_preprocess_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, llm_request: LlmRequest) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 267\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_api_backend\u001b[49m \u001b[38;5;241m==\u001b[39m GoogleLLMVariant\u001b[38;5;241m.\u001b[39mGEMINI_API:\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;66;03m# Using API key from Google AI Studio to call model doesn't support labels.\u001b[39;00m\n\u001b[0;32m    269\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m llm_request\u001b[38;5;241m.\u001b[39mconfig:\n\u001b[0;32m    270\u001b[0m       llm_request\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlabels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\models\\google_llm.py:193\u001b[0m, in \u001b[0;36mGemini._api_backend\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_api_backend\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m GoogleLLMVariant:\n\u001b[0;32m    191\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    192\u001b[0m       GoogleLLMVariant\u001b[38;5;241m.\u001b[39mVERTEX_AI\n\u001b[1;32m--> 193\u001b[0m       \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapi_client\u001b[49m\u001b[38;5;241m.\u001b[39mvertexai\n\u001b[0;32m    194\u001b[0m       \u001b[38;5;28;01melse\u001b[39;00m GoogleLLMVariant\u001b[38;5;241m.\u001b[39mGEMINI_API\n\u001b[0;32m    195\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\functools.py:981\u001b[0m, in \u001b[0;36mcached_property.__get__\u001b[1;34m(self, instance, owner)\u001b[0m\n\u001b[0;32m    979\u001b[0m val \u001b[38;5;241m=\u001b[39m cache\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname, _NOT_FOUND)\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;129;01mis\u001b[39;00m _NOT_FOUND:\n\u001b[1;32m--> 981\u001b[0m     val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    983\u001b[0m         cache[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattrname] \u001b[38;5;241m=\u001b[39m val\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\adk\\models\\google_llm.py:185\u001b[0m, in \u001b[0;36mGemini.api_client\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapi_client\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Client:\n\u001b[0;32m    180\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Provides the api client.\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m  Returns:\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    The api client.\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m      \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mHttpOptions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tracking_headers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\genai\\client.py:219\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[0;32m    216\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    217\u001b[0m     http_options \u001b[38;5;241m=\u001b[39m HttpOptions(base_url\u001b[38;5;241m=\u001b[39mbase_url)\n\u001b[1;32m--> 219\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_api_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_debug_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aio \u001b[38;5;241m=\u001b[39m AsyncClient(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client)\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models \u001b[38;5;241m=\u001b[39m Models(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_api_client)\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\genai\\client.py:265\u001b[0m, in \u001b[0;36mClient._get_api_client\u001b[1;34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m debug_config \u001b[38;5;129;01mand\u001b[39;00m debug_config\u001b[38;5;241m.\u001b[39mclient_mode \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecord\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreplay\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    252\u001b[0m ]:\n\u001b[0;32m    253\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m ReplayApiClient(\n\u001b[0;32m    254\u001b[0m       mode\u001b[38;5;241m=\u001b[39mdebug_config\u001b[38;5;241m.\u001b[39mclient_mode,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    255\u001b[0m       replay_id\u001b[38;5;241m=\u001b[39mdebug_config\u001b[38;5;241m.\u001b[39mreplay_id,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m       http_options\u001b[38;5;241m=\u001b[39mhttp_options,\n\u001b[0;32m    263\u001b[0m   )\n\u001b[1;32m--> 265\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBaseApiClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\genai\\_api_client.py:531\u001b[0m, in \u001b[0;36mBaseApiClient.__init__\u001b[1;34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[0m\n\u001b[0;32m    529\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# Implicit initialization or missing arguments.\u001b[39;00m\n\u001b[0;32m    530\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key:\n\u001b[1;32m--> 531\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    532\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key inputs argument! To use the Google AI API,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    533\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m provide (`api_key`) arguments. To use the Google Cloud API,\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    534\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m provide (`vertexai`, `project` & `location`) arguments.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    535\u001b[0m     )\n\u001b[0;32m    536\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_options\u001b[38;5;241m.\u001b[39mbase_url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://generativelanguage.googleapis.com/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    537\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_http_options\u001b[38;5;241m.\u001b[39mapi_version \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mv1beta\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Missing key inputs argument! To use the Google AI API, provide (`api_key`) arguments. To use the Google Cloud API, provide (`vertexai`, `project` & `location`) arguments."
     ]
    }
   ],
   "source": [
    "# @title Run the Initial Conversation\n",
    "\n",
    "# We need an async function to await our interaction helper\n",
    "async def run_conversation():\n",
    "    await call_agent_async(\"What is the weather like in London?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "    await call_agent_async(\"How about Paris?\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID) # Expecting the tool's error message\n",
    "\n",
    "    await call_agent_async(\"Tell me the weather in New York\",\n",
    "                                       runner=runner,\n",
    "                                       user_id=USER_ID,\n",
    "                                       session_id=SESSION_ID)\n",
    "\n",
    "# Execute the conversation using await in an async context (like Colab/Jupyter)\n",
    "await run_conversation()\n",
    "\n",
    "# --- OR ---\n",
    "\n",
    "# Uncomment the following lines if running as a standard Python script (.py file):\n",
    "# import asyncio\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         asyncio.run(run_conversation())\n",
    "#     except Exception as e:\n",
    "#         print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cf5eeb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 'weather_agent_gpt' created using model 'gemini-2.5-flash'.\n",
      "Session created: App='weather_tutorial_app_gpt', User='user_1_gpt', Session='session_001_gpt'\n",
      "Runner created for agent 'weather_agent_gpt'.\n",
      "\n",
      "--- Testing GPT Agent ---\n",
      "\n",
      ">>> User Query: What's the weather in Tokyo?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m16:11:42 - LiteLLM:ERROR\u001b[0m: vertex_llm_base.py:420 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 416, in get_access_token\n",
      "    _credentials, credential_project_id = self.load_auth(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 110, in load_auth\n",
      "    creds, creds_project_id = self._credentials_from_default_auth(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 156, in _credentials_from_default_auth\n",
      "    return google_auth.default(scopes=scopes)\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\auth\\_default.py\", line 693, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "ERROR:LiteLLM:Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 416, in get_access_token\n",
      "    _credentials, credential_project_id = self.load_auth(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 110, in load_auth\n",
      "    creds, creds_project_id = self._credentials_from_default_auth(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 156, in _credentials_from_default_auth\n",
      "    return google_auth.default(scopes=scopes)\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\auth\\_default.py\", line 693, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
      "\n",
      "❌ Could not create or run GPT agent 'gemini-2.5-flash'. Check API Key and model name. Error: litellm.APIConnectionError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\main.py\", line 525, in acompletion\n",
      "    response = await init_response\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\gemini\\vertex_and_google_ai_studio_gemini.py\", line 1632, in async_completion\n",
      "    _auth_header, vertex_project = await self._ensure_access_token_async(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 479, in _ensure_access_token_async\n",
      "    raise e\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 474, in _ensure_access_token_async\n",
      "    return await asyncify(self.get_access_token)(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\litellm_core_utils\\asyncify.py\", line 57, in wrapper\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 423, in get_access_token\n",
      "    raise e\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 416, in get_access_token\n",
      "    _credentials, credential_project_id = self.load_auth(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 110, in load_auth\n",
      "    creds, creds_project_id = self._credentials_from_default_auth(\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\litellm\\llms\\vertex_ai\\vertex_llm_base.py\", line 156, in _credentials_from_default_auth\n",
      "    return google_auth.default(scopes=scopes)\n",
      "  File \"c:\\Users\\36254\\anaconda3\\envs\\eeg\\lib\\site-packages\\google\\auth\\_default.py\", line 693, in default\n",
      "    raise exceptions.DefaultCredentialsError(_CLOUD_SDK_MISSING_CREDENTIALS)\n",
      "google.auth.exceptions.DefaultCredentialsError: Your default credentials were not found. To set up Application Default Credentials, see https://cloud.google.com/docs/authentication/external/set-up-adc for more information.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# @title Define and Test GPT Agent\n",
    "\n",
    "# Make sure 'get_weather' function from Step 1 is defined in your environment.\n",
    "# Make sure 'call_agent_async' is defined from earlier.\n",
    "\n",
    "# --- Agent using GPT-4o ---\n",
    "weather_agent_gpt = None # Initialize to None\n",
    "runner_gpt = None      # Initialize runner to None\n",
    "\n",
    "try:\n",
    "    weather_agent_gpt = Agent(\n",
    "        name=\"weather_agent_gpt\",\n",
    "        # Key change: Wrap the LiteLLM model identifier\n",
    "        model=LiteLlm(model=MODEL_GPT_4O),\n",
    "        description=\"Provides weather information (using GPT-4o).\",\n",
    "        instruction=\"You are a helpful weather assistant powered by GPT-4o. \"\n",
    "                    \"Use the 'get_weather' tool for city weather requests. \"\n",
    "                    \"Clearly present successful reports or polite error messages based on the tool's output status.\",\n",
    "        tools=[get_weather], # Re-use the same tool\n",
    "    )\n",
    "    print(f\"Agent '{weather_agent_gpt.name}' created using model '{MODEL_GPT_4O}'.\")\n",
    "\n",
    "    # InMemorySessionService is simple, non-persistent storage for this tutorial.\n",
    "    session_service_gpt = InMemorySessionService() # Create a dedicated service\n",
    "\n",
    "    # Define constants for identifying the interaction context\n",
    "    APP_NAME_GPT = \"weather_tutorial_app_gpt\" # Unique app name for this test\n",
    "    USER_ID_GPT = \"user_1_gpt\"\n",
    "    SESSION_ID_GPT = \"session_001_gpt\" # Using a fixed ID for simplicity\n",
    "\n",
    "    # Create the specific session where the conversation will happen\n",
    "    session_gpt = await session_service_gpt.create_session(\n",
    "        app_name=APP_NAME_GPT,\n",
    "        user_id=USER_ID_GPT,\n",
    "        session_id=SESSION_ID_GPT\n",
    "    )\n",
    "    print(f\"Session created: App='{APP_NAME_GPT}', User='{USER_ID_GPT}', Session='{SESSION_ID_GPT}'\")\n",
    "\n",
    "    # Create a runner specific to this agent and its session service\n",
    "    runner_gpt = Runner(\n",
    "        agent=weather_agent_gpt,\n",
    "        app_name=APP_NAME_GPT,       # Use the specific app name\n",
    "        session_service=session_service_gpt # Use the specific session service\n",
    "        )\n",
    "    print(f\"Runner created for agent '{runner_gpt.agent.name}'.\")\n",
    "\n",
    "    # --- Test the GPT Agent ---\n",
    "    print(\"\\n--- Testing GPT Agent ---\")\n",
    "    # Ensure call_agent_async uses the correct runner, user_id, session_id\n",
    "    await call_agent_async(query = \"What's the weather in Tokyo?\",\n",
    "                           runner=runner_gpt,\n",
    "                           user_id=USER_ID_GPT,\n",
    "                           session_id=SESSION_ID_GPT)\n",
    "    # --- OR ---\n",
    "\n",
    "    # Uncomment the following lines if running as a standard Python script (.py file):\n",
    "    # import asyncio\n",
    "    # if __name__ == \"__main__\":\n",
    "    #     try:\n",
    "    #         asyncio.run(call_agent_async(query = \"What's the weather in Tokyo?\",\n",
    "    #                      runner=runner_gpt,\n",
    "    #                       user_id=USER_ID_GPT,\n",
    "    #                       session_id=SESSION_ID_GPT)\n",
    "    #     except Exception as e:\n",
    "    #         print(f\"An error occurred: {e}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Could not create or run GPT agent '{MODEL_GPT_4O}'. Check API Key and model name. Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cc51b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.adk.agents import LlmAgent\n",
    "from google.adk.models.lite_llm import LiteLlm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# 加载 .env 文件\n",
    "load_dotenv()\n",
    "\n",
    "# LiteLLM uses different ENV variables for OpenAI and OpenTelemetry fields.\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://gemini.xzyllmapi.com/v1\"  # SiliconFlow 的 API 地址\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"AIzaSyCxWKvYAy5z7kcVdVw8Leia08GvBAYEgws\"\n",
    "\n",
    "root_agent = LlmAgent(\n",
    "    model=LiteLlm(model=\"gemini-2.5-flash\"),  # 添加 openai/ 前缀\n",
    "    name=\"silicon_flow_agent\",\n",
    "    description=(\n",
    "        \"An assistant powered by SiliconFlow's API.\"\n",
    "    ),\n",
    "    instruction=(\n",
    "        \"You are a helpful assistant using SiliconFlow's language model.\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc000e",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eeg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
